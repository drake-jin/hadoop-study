package wc;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
/**
 * 
@SourceCode

 
@WorkFlow
1. 리듀스 단에는 모든 맵 태스크들의 출력들이 모아서 키인 단어로 정렬을 하고 같은 단어를 갖는 레코드를 하나로 묶는다.
2. 그래서 각 단어별로 그 단어를 기로 하고 그 단어의 빈도수 리스트를 밸류로 만들어서 리듀스의 입력으로 지정한다.
3. 리듀스는 간단하게 리듀스의 밸류로 들어온 빈도수 리스트를 스캔해서 빈도수를 다 더한 값을 구한다.
4. 리듀스의 출력은 결국 해당 단어가 키로 되어있고 밸류는 그 단어의 총 빈도수가 되고 총 빈도수가 최종 출력물이 된다.

@Feature
 - 리듀스도 맵처럼 여러개의 태스크가 동시에 실행될 수 있기 때문에 병렬처리가 가능하다.
 - 한 대의 서버에서 해시맵을 이용하는 프로그래밍 방식과 비교하자면 병렬처리를 할 때 프레임워크가 궂은일을 알아서 해주기 때문에 프로그래머는 맵과 리듀스만 잘 구현해주면 된다.
 - 궂은일 = 1. 맵 메소드로 들어갈 입력 레코드를 준비해주는 것, 
 		   2. 맵 메소드의 출력 레코드들을 받아서 그것들을 리듀스의 입력 레코드로 만들어주는 것  
 
 **/

public class MyWordCount {
	// 정적 클래스로 선언하여 메모리 낭비를 줄이기 위함입니다. 프레임워크가 이 클래스들의 부모 클래스까지 이름까지 알아야 하는데
	// 그런 인자는 현재 정의되어 있지 않기 때문입니다.
	
	// Mapper <K1, V1, K2, V2>  => Reducer<K2, V2, K3, V3> 가 되는 이유
	// 리소스(출력:K1,V1) -> Mapper(Out:K2,V2) -> Reducer(Out:K3, V3) 
	
	
	
	
	// Mapper 의 K1부분과V1이 LongWriteable, Text인 이유는 입력포맷과 관련이 있는데, 일단 어떤 파일이 입력되느냐와 어떤 입력포멧을 사용하느냐에 따라 K1과 V1의 자료형이 결정됩니다.	
	// K2 V2 는 (keyword, count)
	public static class MyMapper extends Mapper<LongWritable, Text, Text, LongWritable>{
		
		private final static LongWritable one = new LongWritable(1);
		private Text word = new Text();
		// Map함수에서 사용할 두 개의 변수를 미리 만들어 둔다. 
		// map함수는 입력 키/벨류 페어에 대해 계속적으로 호출되기 때문에 그 안에서 계속적으로 생성해서 사용하게 되면
		// 가비지 컬렉션에 쌓일 객체들이 늘어나게 됩니다.
		
		
		@Override
		protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, LongWritable>.Context context)
				throws IOException, InterruptedException {
			/*
			String, Long 대신에 LongWritable, Text를 쓰는 이유에는 MapReduce프레임워크에서 WritableComparable인터페이스를 지원해야 사용할 수 있습니다.
			하지만 String과 Long은 저 인터페이스(MapReduce프레임워크 자체 인터페이스 이기 때문에)가 없어서 Text,LongWritable로 대체되어 사용합니다.
			Context는 변환된(결과의) 키/벨류 페어에 대해 값을 써줍니다.
			Key를 LongWritable 을 사용하고 Value를 Text로 받는이유는 앞서 언급한 것 처럼 사실 어떤 입력 포맷을 사용하느냐에 의해 결정됩니다. 
			즉 K1,V1에 따라 입력 파일의 해석을 어떻게 할지 결정이 된다는 것입니다.
			*/
			
			//무슨 역할일까..?
			super.map(key, value, context);
			
			String line = value.toString();
			// 파일의 한줄을 읽는다. Value변수에 들어가 있는 텍스트를 String타입으로 변환한다.
			StringTokenizer tokenizer = new StringTokenizer(line, "\t\n\r\f|,.()>?");
			//토크나이저로 위 언급한 문자열들을 발견할 경우 쪼개어 배열 형태로 만들어버린다.
			while(tokenizer.hasMoreElements()){
				//각 단어에 대해서 작업을 수행합니다.
				word.set(tokenizer.nextToken().toLowerCase());
				// Text객체에 문자열 배열을 하나 삽입한다.
				context.write(word, one);
				// Reducer로 보낼 단어 하나에 대해 카운트 1을 올린다.
			}
		}
	}
	
	// Reducer는 프레임 워크에서 최종적으로 출력하는 녀석.
	public static class MyReducer extends Reducer<Text, LongWritable, Text, LongWritable>{
		
		private LongWritable sumWritable = new LongWritable();
		// map 함수와 비슷하게 성능 개선을 위해 reduce함수에서 필요한 LongWritable의 변수를 미리 하나 만들어 놓고 reduce함수에서 계속 재사용 합니다.
		// 이 변수는 결국 주어진 단어의 빈도수를 합하여 전체 빈도수를 계산하는데 사용합니다.
	
		
		@Override
		// Iterable은 리스트로 출력하겠다는 것이다. 
		// 마지막 Context arg2를 이용하여 프레임워크로 변환된 키/벨류 페어를 출력할 수 있습니다.
		// 맵의 출력 키/벨류 타입과 리듀스의 입력 키/벨류 타입은 일치해야합니다. 그렇기 때문에 K2/V2는 동일하게 사용되야 합니다.
		protected void reduce(Text key, Iterable<LongWritable> values,
				Reducer<Text, LongWritable, Text, LongWritable>.Context context) throws IOException, InterruptedException {
			super.reduce(key, values, context);
			
			long sum =0;
			
			for(LongWritable val : values){
				sum+=val.get();
			// valueㄴ로 넘어온 리스트들을 스캔하면서 값들을 sum변수에 더합니다. 사실 앞 서 map함수의 구현을 보면 val.get()의 값으 항상 1이 됩니다. (map함수에서 계속 1숫자를 넣어주어서)
				
			}
			sumWritable.set(sum);
			//sum에 있는 값을 long자료형에서 MapReduce Framework에서 읽을 수 있도록Writable이 붙은 LongWritable클래스로 변환합니다.
			context.write(key, sumWritable);
			// 출력레코드는 지정된 출력포맷에 맞춰서 지정된 hdfs에 저장될 예정입니다.
		}
	}


/*
	main함수에서는 결국 Configuration과 Job 두 클래스의 인스턴스들을 이용해서 이제 수행하려는 잡의 환경을 설정하고 대략 다음과 같은 일을 수행합니다.
	 - 맵,리듀스 클래스가 존재해야한다.
	 - 입력 파일 위치, 출력 디렉토리 지정, 둘다 HDFS상에 있어야 한다.
	 - 입력/출력 포맷 지정, 이에 대해서는 뒤에서 자세히 설명함.
	 - 최종 출력 키와 벨류타입을 지정해야한다 = 리듀서의 출력 키 벨류
	 - 맵의 출력 키와 벨류 = 리듀서의 입력 키와 벨류
	 
*/	
	

	public static void main(String[] args) throws Exception{
		Configuration conf = new Configuration();
		Job job = new Job(conf, "JobName");

		
		//WordCount class 지정
		job.setJarByClass(MyWordCount.class);
		// Mapper class 지정
		job.setMapperClass(MyMapper.class);
		// Reducer class 지정
		job.setReducerClass(MyReducer.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(LongWritable.class);
		// 두 개의 메소드를 호출하여 잡의 최종 출력 키/밸류 타입을 지정합니다. 
		// 결국 리듀스에서 출력하는 키/밸류의 타입을 여기서 다시 지정해주는 것입니다.
		// 만일 맵의 출력 키/밸류 타입이 리듀스의 출력 키/밸류 값이과 다르다면 Job의 인스턴스의 setMapperOutputKeyClass와 setMapperOutputValueClass 메소드를 호출해 주어야 하는데 WordCount같은경우 
		// 동일하기 때문에 별도로 호출할 필요가 없습니다.
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.waitForCompletion(true);
	}
	/*
	 입력 포맷이 TextInputFormat이면 LongWritable, Text를 K1, V1으로 사용해야하는게 결정되어집니다.
	 -FileInputFormat으로 부터 클래스에서 계승되어졌습니다.
	 -Text파일이 되상이며 .gz 압축처리된 것도 가능합니다.
	 - 라인 하나당 하나의 입력 레코드로 처리됩니다.
	 - 키는 해당 라인의 파일 오프셋이 되고 싶은 타입은 LongWritable 타입이 됩니다.
	 - 밸류는 해당 라인 전체 스트링이고 타입은 Text타입이 됩니다.
	 만일 다른 입력 포맷 클래스를 변경한다면 Job의 setInputFormatClass함수를 호출하면 된다. 
	 */

}
